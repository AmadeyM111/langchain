{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee322f-c613-4cb9-892b-34790fc3607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "import os\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c17fe-b672-4025-8e07-c0575a075961",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375f9bc-3a70-443e-9bd6-ecc890685881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"\\n               You need to return ONLY a valid LaTeX formula for the math formula the user describes or asks for, with no additional text,\\n               no wrapping in $$ or any other special characters, and no escaping of backslashes.  \\n\\n                Here is the user's description of a formula: Varaince of sample mean  \\n               \"\n"
     ]
    }
   ],
   "source": [
    "str_template = \"\"\"\n",
    "               You need to return ONLY a valid LaTeX formula for the math formula the user describes or asks for, with no additional text,\n",
    "               no wrapping in $$ or any other special characters, and no escaping of backslashes.  \n",
    "\n",
    "                Here is the user's description of a formula: {query}  \n",
    "               \"\"\"\n",
    "#Making Prompt Template from one string with variable placeholder {query}\n",
    "give_latex_prompt =  PromptTemplate(template=str_template, \n",
    "                                    input_variables=['query'])\n",
    "specific_prompt = give_latex_prompt.invoke({'query' : 'Varaince of sample'})\n",
    "print(specific_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80610757-527f-494f-86ac-8967990eec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are an assistant for a scientist who writes articles.\\n                    He will ask you for mathematical formulas and expressions or describe them,\\n                    You **must always** return user **only** valid LaTex formula fiting his description wrapped in $$ that and **nothing else**:\\n                  ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Varaince of sample mean', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"You are an assistant for a scientist who writes articles.\n",
    "                    He will ask you for mathematical formulas and expressions or describe them,\n",
    "                    You **must always** return user **only** valid LaTex formula fiting his description wrapped in $$ that and **nothing else**:\n",
    "                  \"\"\"\n",
    "\n",
    "messages = [('system', system_message),\n",
    "            MessagesPlaceholder(\"msgs\")]\n",
    "#making Prompt Template from messages sequence with one placeholder\n",
    "give_latex_prompt_chat = ChatPromptTemplate(messages)\n",
    "\n",
    "specific_prompt = give_latex_prompt_chat.invoke({\"msgs\": [HumanMessage(content='Varaince of sample mean')]})\n",
    "\n",
    "print(specific_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9224a1ef-44d6-471a-85a7-8fefbda7b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how it works in combination with LLM instance\n",
    "token_path = '../data/new_openai_token.txt'\n",
    "with open(token_path, 'rt') as f:\n",
    "    openai_token = f.read()\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_token\n",
    "#token is passed via Enviroment variable defined above\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f0b691-48c9-4ae0-ac98-05c1537eb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\sigma_{\\bar{x}}^2 = \\frac{\\sigma^2}{n}\n"
     ]
    }
   ],
   "source": [
    "#Most simplistic way to define langChain Chain-like object\n",
    "chain = give_latex_prompt | llm\n",
    "\n",
    "result = chain.invoke({'query' : 'Varaince of sample mean'})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5ff18-827e-4199-99b6-d91e1210b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = give_latex_prompt_chat | llm\n",
    "chat_result = chat_chain.invoke({\"msgs\": [HumanMessage(content='Varaince of sample mean')]})\n",
    "print(chat_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f32f4-172d-49e4-94bb-c066129927ca",
   "metadata": {},
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d057cfe2-823b-4751-9c9b-994a8bb79e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Local\\Temp\\ipykernel_12196\\1158537358.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm,  # Ensure 'llm' is properly initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\operatorname{Cov}(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\n"
     ]
    }
   ],
   "source": [
    "#Alternatively we can define Chain like that\n",
    "llm_chain = LLMChain(llm=llm,  # Ensure 'llm' is properly initialized\n",
    "                     prompt=give_latex_prompt)\n",
    "\n",
    "result = llm_chain.invoke({'query' : 'Covariance of two samples of random variables X,Y'})\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b747af65-d7d7-4f5b-8560-e00e8b36d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are great many types of Chains in LangChain\n",
    "#For Example Q/A Chain that can retrieve info from documents\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfd6995-9451-4579-9b4c-38eb53d9e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = \"\"\"\n",
    "               formula 1: Varaince of sample mean,\n",
    "               formula 2: Covariance of two samples of random variables X,Y\n",
    "               formula 3: Poisson distribution\n",
    "               formula 4: Normal Distribution\n",
    "               \"\"\"\n",
    "filepath = '../data/formulas.txt'\n",
    "with open(filepath, 'wt') as f:\n",
    "    f.write(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c3ea50-807b-4a7e-b4d0-ed1dad2bfccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Local\\Temp\\ipykernel_12196\\1668662767.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  retriever = FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()\n"
     ]
    }
   ],
   "source": [
    "# Load documents into a retriever\n",
    "loader = TextLoader(filepath)\n",
    "docs = loader.load()\n",
    "retriever = FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ab8ee9-5f42-405e-9e0f-097768769be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Local\\Temp\\ipykernel_12196\\3888296668.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(\"Give me formula №3 from my list\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula 3 from your list is the Poisson distribution. The probability mass function of a Poisson-distributed random variable is given by:\n",
      "\n",
      "\\[ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\]\n",
      "\n",
      "where:\n",
      "- \\(k\\) is the number of occurrences (a non-negative integer),\n",
      "- \\(e\\) is the base of the natural logarithm,\n",
      "- \\(\\lambda\\) is the average rate (or expected number) of occurrences in the given interval, and\n",
      "- \\(k!\\) is the factorial of \\(k\\).\n"
     ]
    }
   ],
   "source": [
    "# Define the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "# Ask a question\n",
    "response = qa_chain.run(\"Give me formula №3 from my list\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00f2bcf-0851-4b8b-8e0e-73ece8592ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we can put them into one Sequential Chain\n",
    "seq_chain = SimpleSequentialChain(chains=[qa_chain, llm_chain])\n",
    "poisson_latex_formula = seq_chain.run(\"Give me formula №3 from my list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "114558de-53bb-47fd-b1c2-5027d36790f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n"
     ]
    }
   ],
   "source": [
    "print(poisson_latex_formula)\n",
    "#Предположим, что модель не справляется, с удалением двойных \\\\ или все равно ставит $$ $$ вокруг формулы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002a3a5c-957f-42ba-9ece-4b626da1aeee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#There is many ways to fix it, for example like this\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "import re\n",
    "\n",
    "class LaTeXOutputParser(BaseOutputParser):\n",
    "    \"\"\"Custom parser to extract only the LaTeX formula without extra text or escaping issues.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> str:\n",
    "        # Extract formula inside $$...$$ if present\n",
    "        match = re.search(r\"\\$\\$(.*?)\\$\\$\", text, re.DOTALL)\n",
    "        formula = match.group(1) if match else text  # If no match, use full output\n",
    "        \n",
    "        # Fix double backslashes (unescape LaTeX)\n",
    "        formula = formula.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        \n",
    "        return formula.strip()  # Remove any leading/trailing spaces\n",
    "#But also we could define Extra LLM chain to fix output of previous ones :) \n",
    "#MOAR LLM API REQUESTS!!!!\n",
    "\n",
    "# Create LLMChain with the custom parser\n",
    "llm_chain_w_parser = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=give_latex_prompt,\n",
    "    output_parser=LaTeXOutputParser()  # Attach custom parser\n",
    ")\n",
    "\n",
    "seq_chain = SimpleSequentialChain(chains=[qa_chain, llm_chain_w_parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5f86a7-eb74-4ffc-a91e-397a7b39393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_latex_formula = seq_chain.run(\"Give me formula №3 from my list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17521410-3ee6-4eb1-aba9-d9c0c9ff4242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n"
     ]
    }
   ],
   "source": [
    "print(poisson_latex_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e547481-9786-4f00-8d21-788f08a05a9e",
   "metadata": {},
   "source": [
    "## Мы познакомились с простейшими иннструментами LangChain\n",
    "Далее можно расширять наш кругозор в разные стороны. Например:\n",
    "1. Интегрировать уже имеющиеся у нас простые инструменты с еще чем-нибудь и сделать это более похожим на настоящий сервис\n",
    "2. Изучить более продвинутые инструменты, вроде LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349504a6-dee6-4770-af79-94951d18866d",
   "metadata": {},
   "source": [
    "# Интегрируем, то, что у нас есть с Playwright и Telebot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2992f21f-10c7-4a7f-bd4d-f8e864363cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.sync_api import sync_playwright\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f756274f-d265-4350-befc-da5659b1dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Playwright (synchronously) to generate image from LaTeX\n",
    "def generate_latex_image(latex_code):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "\n",
    "        page.goto(\"https://latex2image.joeraut.com/\")\n",
    "        page.fill('textarea[name=\"latex_code\"]', latex_code)\n",
    "        page.click('button:has-text(\"Generate Image\")')\n",
    "        page.wait_for_selector('img.generated-image')\n",
    "\n",
    "        img_url = page.get_attribute('img.generated-image', 'src')\n",
    "        if img_url:\n",
    "            response = requests.get(img_url)\n",
    "            with open(\"../data/latex_image.png\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(\"Image saved as latex_image.png\")\n",
    "        else:\n",
    "            print(\"Error: Could not find generated image.\")\n",
    "\n",
    "        browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "655dbf77-1456-4c95-8192-85d1a0524ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\operatorname{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm,\n",
    "                     prompt=give_latex_prompt)\n",
    "result = llm_chain.invoke({'query' : 'Covariance of two samples of random variables X,Y'})\n",
    "latex_formula = result['text']\n",
    "print(latex_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "026896e5-46e4-45f3-bd9e-d26d0a552372",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_latex_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatex_formula\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m, in \u001b[0;36mgenerate_latex_image\u001b[1;34m(latex_code)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_latex_image\u001b[39m(latex_code):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sync_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m      4\u001b[0m         browser \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m         page \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mnew_page()\n",
      "File \u001b[1;32m~\\Documents\\Programming\\LangChainLearning\\env\\lib\\site-packages\\playwright\\sync_api\\_context_manager.py:47\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_own_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m---> 47\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mPlease use the Async API instead.\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# Create a new fiber for the protocol dispatcher. It will be pumping events\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;66;03m# until the end of times. We will pass control to that fiber every time we\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;66;03m# block while waiting for a response.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgreenlet_main\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "generate_latex_image(latex_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d649e3aa-6a15-43ed-a325-a22cc20b3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting telebot\n",
      "  Downloading telebot-0.0.5-py3-none-any.whl (4.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\arsen\\documents\\programming\\langchainlearning\\env\\lib\\site-packages (from telebot) (2.32.3)\n",
      "Collecting pyTelegramBotAPI\n",
      "  Downloading pytelegrambotapi-4.26.0-py3-none-any.whl (270 kB)\n",
      "     -------------------------------------- 270.5/270.5 KB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arsen\\documents\\programming\\langchainlearning\\env\\lib\\site-packages (from requests->telebot) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsen\\documents\\programming\\langchainlearning\\env\\lib\\site-packages (from requests->telebot) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsen\\documents\\programming\\langchainlearning\\env\\lib\\site-packages (from requests->telebot) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arsen\\documents\\programming\\langchainlearning\\env\\lib\\site-packages (from requests->telebot) (3.10)\n",
      "Installing collected packages: pyTelegramBotAPI, telebot\n",
      "Successfully installed pyTelegramBotAPI-4.26.0 telebot-0.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\arsen\\Documents\\Programming\\LangChainLearning\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install telebot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b583e61a-3b30-4d0b-953d-a6118cc36401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telebot import types\n",
    "from telebot.handler_backends import State, StatesGroup\n",
    "from telebot.storage import StateMemoryStorage\n",
    "from telebot.types import Message\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "with open('../data/tg_bot_token.txt', 'rt') as f:\n",
    "    token = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b4c19-7036-41d1-b797-2f210331c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelegramInterface:\n",
    "    def __init__(self, tg_bot, chain):\n",
    "        self.bot = tg_bot\n",
    "        self.chain = chain\n",
    "\n",
    "        # Register message handler\n",
    "        #for start\n",
    "        @self.bot.message_handler(commands=['start'])\n",
    "        def send_welcome(message: Message):\n",
    "            self.bot.reply_to(message, \"Welcome! Send me formula description and I will generate an image for you.\")\n",
    "        #for ANY text message\n",
    "        @self.bot.message_handler(func=lambda message: True)\n",
    "        def handle_latex_message(message: Message):\n",
    "            #Get latex formula from LLMChain\n",
    "            latex_formula = chain.invoke({'query' : message.text.strip()})\n",
    "            self.bot.send_message(message.chat.id, \"Processing your formula... Please wait ⏳\")\n",
    "            #Generate Image via WebService\n",
    "            image_path = generate_latex_image(latex_code)\n",
    "            \n",
    "            if image_path:\n",
    "                with open(image_path, \"rb\") as img:\n",
    "                    self.bot.send_photo(message.chat.id, img)\n",
    "                os.remove(image_path)  # Cleanup after sending\n",
    "            else:\n",
    "                self.bot.send_message(message.chat.id, \"Error generating image. Please check your LaTeX input.\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Start the bot.\"\"\"\n",
    "        print(\"Bot is running...\")\n",
    "        self.bot.polling(none_stop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
